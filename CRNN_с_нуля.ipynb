{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2hCb-s5Nvkfl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646279795541,"user_tz":-300,"elapsed":50698,"user":{"displayName":"Дмитрий Куценко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlLmaITHRBO-8rtINbs35JitYwkme0zMpo2SpC-A=s64","userId":"04146547092238065676"}},"outputId":"28559294-e030-432f-ea11-0031419d3841"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKwChULrv0BV"},"outputs":[],"source":["import zipfile\n","with zipfile.ZipFile('data_final.zip') as zf:\n","    zf.extractall('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Urc35twGvz-W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646279970501,"user_tz":-300,"elapsed":22394,"user":{"displayName":"Дмитрий Куценко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlLmaITHRBO-8rtINbs35JitYwkme0zMpo2SpC-A=s64","userId":"04146547092238065676"}},"outputId":"246f2afd-f67a-4133-def7-59af2efb5f72"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 60.3 MB 51 kB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 1.4 MB 32.7 MB/s \n","\u001b[K     |████████████████████████████████| 102 kB 26.0 MB/s \n","\u001b[K     |████████████████████████████████| 47.7 MB 1.9 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q opencv-python==4.5.4.60\n","!pip install -q hwb\n","!pip install -qU albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzWP8fbRvz7g"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from google.colab.patches import cv2_imshow\n","import albumentations as albu\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","import json\n","from matplotlib import pyplot as plt\n","from tqdm.notebook import tqdm\n","import random\n","import editdistance\n","from torch import autocast\n","from hwb import HandWrittenBlot"]},{"cell_type":"code","source":["\"\"\"\n","Переворачиваем вертикальные фотки\n","\"\"\"\n","\n","counter_clockwise = ['77148.png', '77141.png', '77144.png', '77143.png', '77153.png', '77151.png', '77142.png', '77149.png', '77150.png', '77146.png']\n","\n","clockwise = ['65827.png', '65876.png', '65851.png', '65837.png', '65870.png', '65826.png', '65852.png', '65864.png', '65835.png', '65822.png',\n","             '65832.png', '65849.png', '65880.png', '65833.png', '65836.png', '65858.png', '65834.png',\n","             '65813.png', '65817.png', '65807.png', '65848.png', '65843.png', '43798.png', '65828.png', '43800.png', '65846.png', '65869.png', '65831.png', '65825.png',\n","             '65830.png', '65818.png', '65863.png', '65855.png', '65868.png', '65860.png', '65815.png', '65881.png', '43804.png', '51319.png', '65844.png',\n","             '65866.png', '65865.png', '65810.png', '65873.png', '65853.png', '65820.png', '43806.png', '43802.png', '65809.png', '65874.png', '65838.png',\n","             '82969.png', '65857.png', '65839.png', '51318.png', '65885.png', '65882.png', '65872.png', '65862.png', '65883.png', '65847.png', '65875.png',\n","             '65840.png', '65842.png', '65805.png', '65812.png', '65850.png', '65878.png', '65821.png', '65871.png', '65845.png', '65823.png']\n","\n","for img_path in counter_clockwise:\n","    img = cv2.imread(f'/content/data_final/train_recognition/images/{img_path}')\n","    img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n","    cv2.imwrite(f'/content/data_final/train_recognition/images/{img_path}', img)\n","\n","for img_path in clockwise:\n","    img = cv2.imread(f'/content/data_final/train_recognition/images/{img_path}')\n","    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n","    cv2.imwrite(f'/content/data_final/train_recognition/images/{img_path}', img)"],"metadata":{"id":"2dHYwVYik9QR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/data_final/train_recognition/labels.csv')\n","file_names, texts = df.file_name.values.tolist(), df.text.values.tolist()"],"metadata":{"id":"rr4p-YV_b8S9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_file_names, new_texts = [], []\n","for i in range(len(df)):\n","    text = texts[i]\n","    if text not in ['2_?', '***.'] and \"<\" not in text:\n","        text = texts[i]\n","        text = text.replace('Ы', 'ы')\n","        text = text.replace('Ь', 'ь')\n","        text = text.replace('}', ']')\n","        text = text.replace('_', ')')\n","        text = text.replace('X', 'x')\n","        text = text.replace('K', 'k')\n","        text = text.replace('<<', '\"')\n","        text = text.replace('>>', '\"')\n","        text = text.replace('A', 'А')\n","        text = text.replace('a', 'а')\n","        text = text.replace('B', 'В')\n","        text = text.replace('C', 'С')\n","        text = text.replace('c', 'с')\n","        text = text.replace('E', 'Е')\n","        text = text.replace('e', 'е')\n","        text = text.replace('H', 'Н')\n","        text = text.replace('M', 'М')\n","        text = text.replace('m', 'м')\n","        text = text.replace('O', 'О')\n","        text = text.replace('o', 'о')\n","        text = text.replace('P', 'Р')\n","        text = text.replace('p', 'р')\n","        text = text.replace('T', 'Т')\n","        text = text.replace('K', 'К')\n","        text = text.replace('k', 'к')\n","        text = text.replace('X', 'Х')\n","        text = text.replace('x', 'х')\n","        new_texts.append(text)\n","        new_file_names.append(file_names[i])"],"metadata":{"id":"66SFIYEhc0vj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y7ZjVE2vz0w"},"outputs":[],"source":["random.seed(1)\n","train_data = [(new_file_names[i], new_texts[i]) for i in range(len(new_texts))]\n","random.shuffle(train_data)\n","\n","split_coef = 0.9\n","\n","train_data_splitted = train_data[:int(len(train_data)*split_coef)]\n","val_data_splitted = train_data[int(len(train_data)*split_coef):]\n","\n","print('train len after split', len(train_data_splitted))\n","print('val len after split', len(val_data_splitted))\n","\n","\n","with open('/content/data_final/train_recognition/train_labels_splitted.json', 'w') as f:\n","    json.dump(dict(train_data_splitted), f)\n","    \n","with open('/content/data_final/train_recognition/val_labels_splitted.json', 'w') as f:\n","    json.dump(dict(val_data_splitted), f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-311kyHovzyD"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","config_json = {\n","    \"alphabet\": ' !\"%\\'()+,-./0123456789:;=?DFGIJLNRSUVWY[]bdfghijlnqrstuvwyz|ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","    \"save_dir\": \"/content/drive/MyDrive/NTO_AI/FINAL/final/weights\",\n","    \"num_epochs\": 500,\n","    \"image\": {\n","        \"width\": 512,\n","        \"height\": 64\n","    },\n","    \"train\": {\n","        \"root_path\": \"/content/data_final/train_recognition/images\",\n","        \"json_path\": \"/content/data_final/train_recognition/train_labels_splitted.json\",\n","        \"batch_size\": 120\n","    },\n","    \"val\": {\n","        \"root_path\": \"/content/data_final/train_recognition/images\",\n","        \"json_path\": \"/content/data_final/train_recognition/val_labels_splitted.json\",\n","        \"batch_size\": 120\n","    },\n","    'seed': 486\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9T2dTddivzvL"},"outputs":[],"source":["def collate_fn(batch):\n","    images, texts, enc_texts = zip(*batch)\n","    images = torch.stack(images, 0)\n","    text_lens = torch.LongTensor([len(text) for text in texts])\n","    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n","    return images, texts, enc_pad_texts, text_lens\n","\n","\n","def get_data_loader(transforms, json_path, root_path, tokenizer, batch_size, drop_last):\n","    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset=dataset,\n","        collate_fn=collate_fn,\n","        batch_size=batch_size,\n","        num_workers=2,)\n","    return data_loader\n","\n","\n","class OCRDataset(Dataset):\n","    def __init__(self, json_path, root_path, tokenizer, transform=None):\n","        super().__init__()\n","        self.transform = transform\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        self.data_len = len(data)\n","\n","        self.img_paths = []\n","        self.texts = []\n","        for img_name, text in data.items():\n","            self.img_paths.append(os.path.join(root_path, img_name))\n","            self.texts.append(text)\n","        self.enc_texts = tokenizer.encode(self.texts)\n","\n","    def __len__(self):\n","        return self.data_len\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        text = self.texts[idx]\n","        enc_text = torch.LongTensor(self.enc_texts[idx])\n","        image = cv2.imread(img_path)\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, text, enc_text\n","\n","\n","class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKh3Lelavzsv"},"outputs":[],"source":["CTC_BLANK = '<BLANK>'\n","\n","def get_char_map(alphabet):\n","    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n","    char_map[CTC_BLANK] = 0\n","    return char_map\n","\n","\n","class Tokenizer:\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def encode(self, word_list):\n","        enc_words = []\n","        for word in word_list:\n","            enc_words.append(\n","                [self.char_map[char] for char in word])\n","        return enc_words\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = ''\n","            for idx, char_enc in enumerate(word):\n","                if (char_enc != self.char_map[CTC_BLANK]\n","                    and not (idx > 0 and char_enc == word[idx - 1])):\n","\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFnaF7yEvzpZ"},"outputs":[],"source":["from albumentations.augmentations.transforms import JpegCompression\t\n","class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class RandomWidth:\n","    def __init__(self, min_scale, max_scale):\n","        self.min_scale = min_scale\n","        self.max_scale = max_scale\n","\n","    def __call__(self, image):\n","        random_scale = random.uniform(self.min_scale, self.max_scale)\n","        image = cv2.resize(image, (int(random_scale*image.shape[1]), image.shape[0]))\n","        if len(image.shape) == 2:\n","            image = np.expand_dims(image, axis=2)\n","        return image\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        h, w, c = image.shape\n","        new_image = np.zeros((self.height, self.width, c), np.uint8)\n","        scale = self.height / h\n","        if int(w*scale) <= self.width:\n","            image = cv2.resize(image, (int(w*scale), self.height), interpolation=cv2.INTER_LINEAR)\n","            new_image[:, :image.shape[1], :] = image\n","        else:\n","            new_height = int(self.height * (self.width / int(w*scale)))\n","            image = cv2.resize(image, (self.width, new_height), interpolation=cv2.INTER_LINEAR)\n","            new_image[(self.height-new_height)//2:-((self.height-new_height)-(self.height-new_height)//2), :image.shape[1], :] = image\n","            \n","        return new_image\n","\n","class HandwrittenBlots:\n","    def __init__(self, p=0.5):\n","        self.rectangle_info = {'x': (None, None),'y': (None, None),'h': (None, None),'w': (None, None)}\n","        self.blot_params = {'incline': (-10, 10),'intensivity': (0.1, 0.17),'transparency': (0.2, 0.3),'count': (1, 3)}\n","        self.blots = HandWrittenBlot(self.rectangle_info, self.blot_params)\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            new_img = self.blots(image)\n","        else:\n","            new_img = image\n","        return new_img\n","\n","class albu_aug:\n","    def __init__(self):\n","        self.aug_list = albu.Compose([\n","                                      albu.CoarseDropout(p=0.3),\n","                                      albu.Rotate(limit=5, p=0.6),\n","                                      albu.ToGray(p=0.15),\n","                                      albu.RandomBrightnessContrast(brightness_limit=0.11,\n","                                                                    contrast_limit=0.11, p=0.4),\n","                                      albu.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.15),\n","                                      albu.GridDistortion(p=0.2),\n","                                      albu.OpticalDistortion(p=0.2),\n","                                      albu.HueSaturationValue(hue_shift_limit=7, sat_shift_limit=7,\n","                                                              val_shift_limit=7, p=0.3),\n","                                      albu.RandomGamma(gamma_limit=(85, 115), p=0.3),\n","                                      albu.OneOf([\n","                                                  albu.Blur(blur_limit=5, p=0.1),\n","                                                  albu.MedianBlur(blur_limit=5, p=0.1)\n","                                      ]),\n","                                      albu.GaussNoise(var_limit=(20.0, 50.0), p=0.2),\n","                                      albu.RGBShift(r_shift_limit=7, g_shift_limit=7, b_shift_limit=7, p=0.3),\n","                                      albu.JpegCompression(quality_lower=80, p=0.15),\n","                                      albu.ChannelShuffle(p=0.07)])\n","\n","    def __call__(self, image):\n","        image = self.aug_list(image=image)['image']\n","        image[:, :, 0] -= image[:, :, 0].min()\n","        image[:, :, 1] -= image[:, :, 1].min()\n","        image[:, :, 2] -= image[:, :, 2].min()\n","        return image\n","\n","\n","class Reduce_thickness:\n","    def __init__(self, p=1):\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            kernel = np.ones((4,4), np.uint8)\n","            return cv2.dilate(image, kernel, iterations=1)\n","        else:\n","            return image\n","\n","\n","class Stretch:\n","    def __init__(self, p=1):\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            stretch = (random.random() - 0.5)\n","            wStretched = max(int(image.shape[1] * (1 + stretch)), 1)\n","            img = cv2.resize(image, (wStretched, image.shape[0]))\n","        return image\n","\n","\n","class CutBottom:\n","    def __init__(self, cut_percent = 0.35, p=1, **kwargs):\n","        self.cut_percent = cut_percent\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            height, width = image.shape[:2]\n","            remove_h = min(-1, -int(height * random.uniform(0, self.cut_percent)))\n","            image = image[:remove_h]\n","        return image\n","\n","\n","def get_train_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        HandwrittenBlots(p=0.2),\n","        CutBottom(cut_percent=0.15, p=0.2),\n","        albu_aug(),\n","        RandomWidth(0.8, 1.5),\n","        Reduce_thickness(p=0.07),\n","        Stretch(p=0.4),\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor()\n","    ])\n","    return transforms\n","\n","\n","def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor()\n","    ])\n","    return transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OXL3FNZEvzjq"},"outputs":[],"source":["def get_resnet34_new_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=pretrained)\n","    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu,\n","              m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size, hidden_size, num_layers,\n","            dropout=dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(self, number_class_symbols, time_feature_count=256, lstm_hidden=400, lstm_len=3, pretrained=True):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_new_backbone(pretrained=pretrained)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((time_feature_count, time_feature_count))\n","        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden*2, 300),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(300, number_class_symbols))\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyVhoToEvzgh"},"outputs":[],"source":["def get_metrics(true_texts, pred_texts):\n","    numCharErr = 0\n","    numCharTotal = 0\n","    numStringOK = 0\n","    numStringTotal = 0\n","    word_eds, word_true_lens = [], []\n","    for i in range(len(pred_texts)):\n","        pred = pred_texts[i]\n","        true = true_texts[i]\n","    \n","        numStringOK += 1 if true == pred else 0\n","        numStringTotal += 1\n","        dist = editdistance.eval(pred, true)\n","        numCharErr += dist\n","        numCharTotal += len(true)\n","        \n","        pred_words = pred.split()\n","        true_words = true.split()\n","        word_eds.append(editdistance.eval(pred_words, true_words))\n","        word_true_lens.append(len(true_words))\n","    \n","    charErrorRate = numCharErr / numCharTotal\n","    wordErrorRate = sum(word_eds) / sum(word_true_lens)\n","    stringAccuracy = numStringOK / numStringTotal\n","    return charErrorRate, wordErrorRate, stringAccuracy\n","\n","\n","def val_loop(data_loader, model, tokenizer, device):\n","    model.to(DEVICE)\n","    all_texts, all_preds = [], []\n","    for images, texts, _, _ in data_loader:\n","        batch_size = len(texts)\n","        text_preds = predict(images, model, tokenizer, device)\n","        all_texts.append(texts)\n","        all_preds.append(text_preds)\n","    all_texts, all_preds = np.concatenate(all_texts), np.concatenate(all_preds)\n","    avg_cer, wer, acc = get_metrics(all_texts, all_preds)\n","    print(f'Validation CER: {avg_cer:.4f}  WER: {wer:.4f}  ACCURACY: {acc:.4f}')\t \n","    return avg_cer\n","\n","\n","def train_loop(data_loader, model, criterion, optimizer, scheduler, epoch, gradient_accumulation_steps=4):\n","    optimizer.zero_grad()\n","    loss_avg = AverageMeter()\n","    model.to(DEVICE)\n","    model.train()\n","    scaled_loss = 0\n","    for step, (images, texts, enc_pad_texts, text_lens) in tqdm(enumerate(data_loader), total=len(data_loader)):\n","        images = images.to(DEVICE)\n","        batch_size = len(texts)\n","\n","        with autocast('cuda'):\n","            output = model(images)\n","            output_lenghts = torch.full(\n","                size=(output.size(1),),\n","                fill_value=output.size(0),\n","                dtype=torch.long)\n","            loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n","            scaled_loss += loss.item()\n","            loss_avg.update(loss.item(), batch_size)\n","            loss.backward()\n","\n","            if (step + 1) % gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                scaled_loss = 0\n","\n","    for param_group in optimizer.param_groups:\n","        lr = param_group['lr']\n","    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n","    return loss_avg.avg\n","\n","\n","def predict(images, model, tokenizer, device):\n","    model.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model(images)\n","    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    text_preds = tokenizer.decode(pred)\n","    return text_preds\n","\n","\n","def get_loaders(tokenizer, config):\n","    train_transforms = get_train_transforms(\n","        height=config['image']['height'],\n","        width=config['image']['width'])\n","    train_loader = get_data_loader(\n","        json_path=config['train']['json_path'],\n","        root_path=config['train']['root_path'],\n","        transforms=train_transforms,\n","        tokenizer=tokenizer,\n","        batch_size=config['train']['batch_size'],\n","        drop_last=True)\n","    val_transforms = get_val_transforms(\n","        height=config['image']['height'],\n","        width=config['image']['width'])\n","    val_loader = get_data_loader(\n","        transforms=val_transforms,\n","        json_path=config['val']['json_path'],\n","        root_path=config['val']['root_path'],\n","        tokenizer=tokenizer,\n","        batch_size=config['val']['batch_size'],\n","        drop_last=False)\n","    return train_loader, val_loader\n","\n","\n","def train(config):\n","    tokenizer = Tokenizer(config['alphabet'])\n","    os.makedirs(config['save_dir'], exist_ok=True)\n","    train_loader, val_loader = get_loaders(tokenizer, config)\n","\n","    model = CRNN(number_class_symbols=tokenizer.get_num_chars(), pretrained=True)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.3, patience=3)\n","    cer_best = val_loop(val_loader, model, tokenizer, DEVICE)\n","    for epoch in range(100):\n","        loss_avg = train_loop(train_loader, model, criterion, optimizer, scheduler, epoch, gradient_accumulation_steps=1)\n","        scheduler.step(loss_avg)\n","\n","        cer_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n","        if cer_avg < cer_best:\n","            cer_best = cer_avg\n","            torch.save(model.state_dict(), 'recognition_V4_1.pth')\n","            print('Model weights saved')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIgU8tauvzdM"},"outputs":[],"source":["train(config_json)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"CRNN_с_нуля.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}