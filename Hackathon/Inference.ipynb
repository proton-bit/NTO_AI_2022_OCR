{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpyYHL9iztpn0+0TH/6gRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S8QZg7S2BqjD"},"outputs":[],"source":["import json\n","import os\n","import sys\n","import warnings\n","\n","import cv2\n","import numpy as np\n","from PIL import Image, ImageDraw, ImageFont\n","from tqdm import tqdm\n","\n","import logging\n","\n","warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultPredictor\n","from ctcdecode import CTCBeamDecoder\n","\n","logger = logging.getLogger(\"detectron2\")\n","logger.setLevel(logging.CRITICAL)\n","\n","\n","TEST_IMAGES_PATH, SAVE_PATH = sys.argv[1:]\n","\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('texts_big.txt', 'r') as f:\n","    ALL_TEXTS = set(f.read().split('\\n'))\n","\n","SEGM_MODEL_PATH = \"segmentation_big.pth\"\n","OCR_MODEL_PATH = \"recognition_V3_5.ckpt\"\n","\n","\n","CONFIG_JSON = {\n","    \"alphabet\": ' !|\"\\'()+,-./0123456789:;=?IN[]ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","    \"image\": {\"width\": 512, \"height\": 64}}\n","\n","\n","def get_contours_from_mask(mask, min_area=5):\n","    contours, hierarchy = cv2.findContours(\n","        mask.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","    contour_list = []\n","    for contour in contours:\n","        if cv2.contourArea(contour) >= min_area:\n","            contour_list.append(contour)\n","    return contour_list\n","\n","\n","def get_larger_contour(contours):\n","    larger_area = 0\n","    larger_contour = None\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area > larger_area:\n","            larger_contour = contour\n","            larger_area = area\n","    return larger_contour\n","\n","\n","class SEGMpredictor:\n","    def __init__(self, model_path):\n","        cfg = get_cfg()\n","        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n","        cfg.MODEL.WEIGHTS = model_path\n","        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.07\n","        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n","        cfg.INPUT.FORMAT = \"BGR\"\n","        cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n","        cfg.INPUT.MIN_SIZE_TEST = 900\n","        cfg.INPUT.MAX_SIZE_TEST = 1500\n","\n","        self.predictor = DefaultPredictor(cfg)\n","\n","    def __call__(self, img):\n","        outputs = self.predictor(img)\n","        prediction = outputs[\"instances\"].pred_masks.cpu().numpy()\n","        contours = []\n","        for pred in prediction:\n","            contour_list = get_contours_from_mask(pred)\n","            contours.append(get_larger_contour(contour_list))\n","        return contours\n","\n","\n","def get_char_map(alphabet):\n","    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n","    char_map[\"<BLANK>\"] = 0\n","    return char_map\n","\n","\n","class Tokenizer:\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = \"\"\n","            for idx, char_enc in enumerate(word):\n","                if char_enc != self.char_map[\"<BLANK>\"] and not (idx > 0 and char_enc == word[idx - 1]):\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words\n","\n","\n","class TokenizerBS:\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","        self.train_lm_decoder = CTCBeamDecoder(\n","                                                '_ !|\"\\'()+,-./0123456789:;=?IN[]ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","                                                model_path=f\"language_model_big.gz\",\n","                                                alpha=0.8, beta=1.5,\n","                                                cutoff_top_n=50, cutoff_prob=1.0,\n","                                                beam_width=100, num_processes=6,\n","                                                blank_id=0, log_probs_input=True)\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        beam_results, beam_scores, time_steps, out_lens = self.train_lm_decoder.decode(enc_word_list)\n","        text_preds = ['' for x in range(len(beam_results))]\n","        for i in range(len(beam_results)):\n","            hyp_len = out_lens[i][0]\n","            for x in range(int(hyp_len)):\n","                if beam_results[i, 0, x] > 0:\n","                    text_preds[i] += '_ !|\"\\'()+,-./0123456789:;=?IN[]ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'[beam_results[i, 0, x]]\n","        return text_preds\n","\n","\n","class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        h, w, c = image.shape\n","        new_image = np.zeros((self.height, self.width, c), np.uint8)\n","        scale = self.height / h\n","        if int(w * scale) <= self.width:\n","            image = cv2.resize(image, (int(w * scale), self.height), interpolation=cv2.INTER_LINEAR)\n","            new_image[:, :image.shape[1], :] = image\n","        else:\n","            new_height = int(self.height * (self.width / int(w * scale)))\n","            image = cv2.resize(image, (self.width, new_height), interpolation=cv2.INTER_LINEAR)\n","            new_image[(self.height - new_height) // 2:-((self.height - new_height) - (self.height - new_height) // 2),\n","            :image.shape[1], :] = image\n","\n","        return new_image\n","\n","\n","def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose(\n","        [\n","            ImageResize(height, width),\n","            MoveChannels(to_channels_first=True),\n","            Normalize(),\n","            ToTensor(),\n","        ]\n","    )\n","    return transforms\n","\n","\n","def get_resnet34_new_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=False)\n","    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu,\n","              m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size, hidden_size, num_layers,\n","            dropout=dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(self, number_class_symbols, time_feature_count=256, lstm_hidden=512, lstm_len=3, pretrained=True):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_new_backbone(pretrained=pretrained)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((lstm_hidden, time_feature_count))\n","        self.bilstm = BiLSTM(lstm_hidden, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden*2, 368),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(368, number_class_symbols))\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x\n","\n","\n","def predict(images, model, tokenizer, tokenizer_bs, device):\n","    model.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model(images)\n","    output1 = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    output2 = output.transpose(0, 1).detach()\n","    preds_grid = tokenizer.decode(output1)\n","    preds_beam = tokenizer_bs.decode(output2)\n","\n","    final_preds = []\n","    for i in range(len(preds_grid)):\n","        if preds_grid[i] in ALL_TEXTS or preds_beam[i] not in ALL_TEXTS:\n","            final_preds.append(preds_grid[i])\n","        else:\n","            final_preds.append(preds_beam[i])\n","\n","    return final_preds\n","\n","\n","class InferenceTransform:\n","    def __init__(self, height, width):\n","        self.transforms = get_val_transforms(height, width)\n","\n","    def __call__(self, images):\n","        transformed_images = []\n","        for image in images:\n","            image = self.transforms(image)\n","            transformed_images.append(image)\n","        transformed_tensor = torch.stack(transformed_images, 0)\n","        return transformed_tensor\n","\n","\n","class OcrPredictor:\n","    def __init__(self, model_path, config, device=\"cuda\"):\n","        self.tokenizer_bs = TokenizerBS(config[\"alphabet\"])\n","        self.tokenizer = Tokenizer(config['alphabet'])\n","        self.device = torch.device(device)\n","        # load model\n","        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n","        self.model.load_state_dict(torch.load(model_path))\n","        self.model.to(self.device)\n","\n","        self.transforms = InferenceTransform(\n","            height=config[\"image\"][\"height\"],\n","            width=config[\"image\"][\"width\"])\n","\n","    def __call__(self, images):\n","        if isinstance(images, (list, tuple)):\n","            one_image = False\n","        elif isinstance(images, np.ndarray):\n","            images = [images]\n","            one_image = True\n","        else:\n","            raise Exception(\n","                f\"Input must contain np.ndarray, \"\n","                f\"tuple or list, found {type(images)}.\")\n","\n","        images = self.transforms(images)\n","        pred = predict(images, self.model, self.tokenizer, self.tokenizer_bs, self.device)\n","\n","        if one_image:\n","            return pred[0]\n","        else:\n","            return pred\n","\n","\n","def crop_img_by_polygon(img, polygon):\n","    pts = np.array(polygon)\n","    rect = cv2.boundingRect(pts)\n","    x, y, w, h = rect\n","    croped = img[y : y + h, x : x + w].copy()\n","    pts = pts - pts.min(axis=0)\n","    mask = np.zeros(croped.shape[:2], np.uint8)\n","    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n","    dst = cv2.bitwise_and(croped, croped, mask=mask)\n","    return dst\n","\n","\n","class PiepleinePredictor:\n","    def __init__(self, segm_model_path, ocr_model_path, ocr_config):\n","        self.segm_predictor = SEGMpredictor(model_path=segm_model_path)\n","        self.ocr_predictor = OcrPredictor(\n","            model_path=ocr_model_path,\n","            config=CONFIG_JSON\n","        )\n","\n","    def __call__(self, img):\n","        output = {'predictions': []}\n","        contours = self.segm_predictor(img)\n","        crops = []\n","        new_contours = []\n","        for contour in contours:\n","            if contour is not None:\n","                crop = crop_img_by_polygon(img, contour)\n","                crops.append(crop)\n","                new_contours.append(contour)\n","\n","        pred_texts = self.ocr_predictor(crops)\n","        for j in range(len(pred_texts)):\n","            output['predictions'].append(\n","                {'polygon': [[int(i[0][0]), int(i[0][1])] for i in new_contours[j]],\n","                 'text': pred_texts[j]})\n","        return output\n","\n","\n","def main():\n","    pipeline_predictor = PiepleinePredictor(\n","        segm_model_path=SEGM_MODEL_PATH,\n","        ocr_model_path=OCR_MODEL_PATH,\n","        ocr_config=CONFIG_JSON,\n","    )\n","    pred_data = {}\n","    for img_name in tqdm(os.listdir(TEST_IMAGES_PATH)):\n","        image = cv2.imread(os.path.join(TEST_IMAGES_PATH, img_name))\n","        pred_data[img_name] = pipeline_predictor(image)\n","\n","    with open(SAVE_PATH, \"w\") as f:\n","        json.dump(pred_data, f)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}