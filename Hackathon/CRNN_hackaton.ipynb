{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CRNN_hackaton.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPE2K1+XSKO8y9+5xL3D1Mz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Взял обученную mask-rcnn, заинференсили на всем тренировочном датасете, и соотнесли получившиеся полигоны с разметкой, и добавили это все к тренировочному датасету, получилось: 120к+100к(новых)=220к фото в датасете\n","\n","https://drive.google.com/file/d/1IhDDTEmz_TxPN91i2Jcn_YxR25guY-Oq/view?usp=sharing"],"metadata":{"id":"oIZY8Z4qU2oo"}},{"cell_type":"code","source":["!mkdir train_recognition\n","import zipfile\n","with zipfile.ZipFile('train_recognition_V3.zip') as zf:\n","    zf.extractall('train_recognition')"],"metadata":{"id":"VKwChULrv0BV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q opencv-python==4.5.4.60\n","!pip install -q hwb\n","!pip install -qU albumentations"],"metadata":{"id":"Urc35twGvz-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from google.colab.patches import cv2_imshow\n","import albumentations as albu\n","\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os\n","import json\n","from matplotlib import pyplot as plt\n","from tqdm.notebook import tqdm\n","import random\n","import editdistance\n","from torch import autocast\n","from hwb import HandWrittenBlot"],"metadata":{"id":"DzWP8fbRvz7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_csv('/content/train_recognition/labels.csv')\n","train_data = [(train_df.file_name.values[i], train_df.text.values[i]) for i in range(len(train_df))]\n","train_data = [train_data[i] for i in range(len(train_data)) if 'D' not in train_data[i][1] and\n","              'Ы' not in train_data[i][1] and 'Ь' not in train_data[i][1] and 'z' not in train_data[i][1] and 'b' not in train_data[i][1] and 'f' not in train_data[i][1] and '*' not in train_data[i][1] and 'S' not in train_data[i][1] and 'V' not in train_data[i][1] and '}' not in train_data[i][1] and 'z' not in train_data[i][1] and\n","              'L' not in train_data[i][1] and '_' not in train_data[i][1] and 'h' not in train_data[i][1] and 'l' not in train_data[i][1] and 'r' not in train_data[i][1] and 's' not in train_data[i][1] and 't' not in train_data[i][1] and '<' not in train_data[i][1] and '>' not in train_data[i][1] and 'B' not in train_data[i][1] and 'O' not in train_data[i][1] and 'a' not in train_data[i][1] and 'c' not in train_data[i][1] and 'n' not in train_data[i][1] and 'Щ' not in train_data[i][1]]\n","random.seed(1)\n","random.shuffle(train_data)\n","print('train len', len(train_data))\n","\n","split_coef = 0.8\n","train_len = int(len(train_data)*split_coef)\n","\n","train_data_splitted = train_data[:train_len]\n","val_data_splitted = train_data[train_len:]\n","\n","print('train len after split', len(train_data_splitted))\n","print('val len after split', len(val_data_splitted))\n","\n","\n","with open('train_recognition/train_labels_splitted.json', 'w') as f:\n","    json.dump(dict(train_data_splitted), f)\n","    \n","with open('train_recognition/val_labels_splitted.json', 'w') as f:\n","    json.dump(dict(val_data_splitted), f)"],"metadata":{"id":"1y7ZjVE2vz0w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","config_json = {\n","    \"alphabet\": ' !|\"\\'()+,-./0123456789:;=?IN[]ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","    \"save_dir\": \"/content/drive/MyDrive/NTO_AI/Hackathon/V3/weights\",\n","    \"num_epochs\": 500,\n","    \"image\": {\n","        \"width\": 512,\n","        \"height\": 64\n","    },\n","    \"train\": {\n","        \"root_path\": \"/content/train_recognition/images\",\n","        \"json_path\": \"train_recognition/train_labels_splitted.json\",\n","        \"batch_size\": 64\n","    },\n","    \"val\": {\n","        \"root_path\": \"/content/train_recognition/images\",\n","        \"json_path\": \"train_recognition/val_labels_splitted.json\",\n","        \"batch_size\": 80\n","    }\n","}"],"metadata":{"id":"-311kyHovzyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    images, texts, enc_texts = zip(*batch)\n","    images = torch.stack(images, 0)\n","    text_lens = torch.LongTensor([len(text) for text in texts])\n","    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n","    return images, texts, enc_pad_texts, text_lens\n","\n","\n","def get_data_loader(\n","    transforms, json_path, root_path, tokenizer, batch_size, drop_last\n","):\n","    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n","    data_loader = torch.utils.data.DataLoader(\n","        dataset=dataset,\n","        collate_fn=collate_fn,\n","        batch_size=batch_size,\n","        num_workers=2,\n","    )\n","    return data_loader\n","\n","\n","class OCRDataset(Dataset):\n","    def __init__(self, json_path, root_path, tokenizer, transform=None):\n","        super().__init__()\n","        self.transform = transform\n","        with open(json_path, 'r') as f:\n","            data = json.load(f)\n","        self.data_len = len(data)\n","\n","        self.img_paths = []\n","        self.texts = []\n","        for img_name, text in data.items():\n","            self.img_paths.append(os.path.join(root_path, img_name))\n","            self.texts.append(text)\n","        self.enc_texts = tokenizer.encode(self.texts)\n","\n","    def __len__(self):\n","        return self.data_len\n","\n","    def __getitem__(self, idx):\n","        img_path = self.img_paths[idx]\n","        text = self.texts[idx]\n","        enc_text = torch.LongTensor(self.enc_texts[idx])\n","        image = cv2.imread(img_path)\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, text, enc_text\n","\n","\n","class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"metadata":{"id":"9T2dTddivzvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CTC_BLANK = '<BLANK>'\n","\n","def get_char_map(alphabet):\n","    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n","    char_map[CTC_BLANK] = 0\n","    return char_map\n","\n","\n","class Tokenizer:\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def encode(self, word_list):\n","        enc_words = []\n","        for word in word_list:\n","            enc_words.append(\n","                [self.char_map[char] for char in word])\n","        return enc_words\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = ''\n","            for idx, char_enc in enumerate(word):\n","                if (char_enc != self.char_map[CTC_BLANK]\n","                    and not (idx > 0 and char_enc == word[idx - 1])):\n","\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words"],"metadata":{"id":"uKh3Lelavzsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from albumentations.augmentations.transforms import JpegCompression\t\n","class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class RandomWidth:\n","    def __init__(self, min_scale, max_scale):\n","        self.min_scale = min_scale\n","        self.max_scale = max_scale\n","\n","    def __call__(self, image):\n","        random_scale = random.uniform(self.min_scale, self.max_scale)\n","        image = cv2.resize(image, (int(random_scale*image.shape[1]), image.shape[0]))\n","        if len(image.shape) == 2:\n","            image = np.expand_dims(image, axis=2)\n","        return image\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        h, w, c = image.shape\n","        new_image = np.zeros((self.height, self.width, c), np.uint8)\n","        scale = self.height / h\n","        if int(w*scale) <= self.width:\n","            image = cv2.resize(image, (int(w*scale), self.height), interpolation=cv2.INTER_LINEAR)\n","            new_image[:, :image.shape[1], :] = image\n","        else:\n","            new_height = int(self.height * (self.width / int(w*scale)))\n","            image = cv2.resize(image, (self.width, new_height), interpolation=cv2.INTER_LINEAR)\n","            new_image[(self.height-new_height)//2:-((self.height-new_height)-(self.height-new_height)//2), :image.shape[1], :] = image\n","            \n","        return new_image\n","\n","class HandwrittenBlots:\n","    def __init__(self, p=0.5):\n","        self.rectangle_info = {'x': (None, None),'y': (None, None),'h': (None, None),'w': (None, None)}\n","        self.blot_params = {'incline': (-10, 10),'intensivity': (0.1, 0.2),'transparency': (0.2, 0.3),'count': (1, 3)}\n","        self.blots = HandWrittenBlot(self.rectangle_info, self.blot_params)\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            new_img = self.blots(image)\n","        else:\n","            new_img = image\n","        return new_img\n","\n","class albu_aug:\n","    def __init__(self):\n","        self.aug_list = albu.Compose([\n","                                      albu.CoarseDropout(p=0.3),\n","                                      albu.Rotate(limit=5, p=0.6),\n","                                      albu.ToGray(p=0.2),\n","                                      albu.RandomBrightnessContrast(brightness_limit=0.1,\n","                                                                    contrast_limit=0.1, p=0.4),\n","                                      albu.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.2),\n","                                      albu.GridDistortion(p=0.2),\n","                                      albu.OpticalDistortion(p=0.2),\n","                                      albu.HueSaturationValue(hue_shift_limit=7, sat_shift_limit=7,\n","                                                              val_shift_limit=7, p=0.3),\n","                                      albu.RandomGamma(gamma_limit=(90, 110), p=0.3),\n","                                      albu.OneOf([\n","                                                  albu.Blur(blur_limit=5, p=0.1),\n","                                                  albu.MedianBlur(blur_limit=5, p=0.1)\n","                                      ]),\n","                                      albu.GaussNoise(var_limit=(20.0, 50.0), p=0.2),\n","                                      albu.RGBShift(r_shift_limit=7, g_shift_limit=7, b_shift_limit=7, p=0.3),\n","                                      albu.JpegCompression(quality_lower=75, p=0.15)])\n","\n","    def __call__(self, image):\n","        image = self.aug_list(image=image)['image']\n","        image[:, :, 0] -= image[:, :, 0].min()\n","        image[:, :, 1] -= image[:, :, 1].min()\n","        image[:, :, 2] -= image[:, :, 2].min()\n","        return image\n","\n","\n","class Reduce_thickness:\n","    def __init__(self, p=1):\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            kernel = np.ones((4,4), np.uint8)\n","            return cv2.dilate(image, kernel, iterations=1)\n","        else:\n","            return image\n","\n","\n","class Stretch:\n","    def __init__(self, p=1):\n","        self.p = p\n","\n","    def __call__(self, image):\n","        if random.random() < self.p:\n","            stretch = (random.random() - 0.5)\n","            wStretched = max(int(image.shape[1] * (1 + stretch)), 1)\n","            img = cv2.resize(image, (wStretched, image.shape[0]))\n","        return image\n","\n","\n","def get_train_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        HandwrittenBlots(p=0.25),\n","        albu_aug(),\n","        RandomWidth(0.85, 1.15),\n","        Reduce_thickness(p=0.07),\n","        Stretch(p=0.2),\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor()\n","    ])\n","    return transforms\n","\n","\n","def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose([\n","        ImageResize(height, width),\n","        MoveChannels(to_channels_first=True),\n","        Normalize(),\n","        ToTensor()\n","    ])\n","    return transforms"],"metadata":{"id":"FFnaF7yEvzpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_resnet34_new_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=pretrained)\n","    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu,\n","              m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size, hidden_size, num_layers,\n","            dropout=dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(self, number_class_symbols, time_feature_count=256, lstm_hidden=512, lstm_len=3, pretrained=True):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_new_backbone(pretrained=pretrained)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((lstm_hidden, time_feature_count))\n","        self.bilstm = BiLSTM(lstm_hidden, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden*2, 368),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(368, number_class_symbols))\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x"],"metadata":{"id":"OXL3FNZEvzjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_metrics(true_texts, pred_texts):\n","    numCharErr = 0\n","    numCharTotal = 0\n","    numStringOK = 0\n","    numStringTotal = 0\n","    word_eds, word_true_lens = [], []\n","    for i in range(len(pred_texts)):\n","        pred = pred_texts[i]\n","        true = true_texts[i]\n","    \n","        numStringOK += 1 if true == pred else 0\n","        numStringTotal += 1\n","        dist = editdistance.eval(pred, true)\n","        numCharErr += dist\n","        numCharTotal += len(true)\n","        \n","        pred_words = pred.split()\n","        true_words = true.split()\n","        word_eds.append(editdistance.eval(pred_words, true_words))\n","        word_true_lens.append(len(true_words))\n","    \n","    charErrorRate = numCharErr / numCharTotal\n","    wordErrorRate = sum(word_eds) / sum(word_true_lens)\n","    stringAccuracy = numStringOK / numStringTotal\n","    return charErrorRate, wordErrorRate, stringAccuracy\n","\n","\n","def val_loop(data_loader, model, tokenizer, device):\n","    model.to(DEVICE)\n","    all_texts, all_preds = [], []\n","    for images, texts, _, _ in data_loader:\n","        batch_size = len(texts)\n","        text_preds = predict(images, model, tokenizer, device)\n","        all_texts.append(texts)\n","        all_preds.append(text_preds)\n","    all_texts, all_preds = np.concatenate(all_texts), np.concatenate(all_preds)\n","    avg_cer, wer, acc = get_metrics(all_texts, all_preds)\n","    print(f'Validation CER: {avg_cer:.4f}  WER: {wer:.4f}  ACCURACY: {acc:.4f}')\t \n","    return avg_cer\n","\n","\n","def train_loop(data_loader, model, criterion, optimizer, scheduler, epoch, gradient_accumulation_steps=4):\n","    optimizer.zero_grad()\n","    loss_avg = AverageMeter()\n","    model.to(DEVICE)\n","    model.train()\n","    scaled_loss = 0\n","    for step, (images, texts, enc_pad_texts, text_lens) in tqdm(enumerate(data_loader), total=len(data_loader)):\n","        images = images.to(DEVICE)\n","        batch_size = len(texts)\n","\n","        with autocast('cuda'):\n","            output = model(images)\n","            output_lenghts = torch.full(\n","                size=(output.size(1),),\n","                fill_value=output.size(0),\n","                dtype=torch.long)\n","            loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n","            scaled_loss += loss.item()\n","            loss_avg.update(loss.item(), batch_size)\n","            loss.backward()\n","\n","            if (step + 1) % gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                scaled_loss = 0\n","\n","    for param_group in optimizer.param_groups:\n","        lr = param_group['lr']\n","    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n","    return loss_avg.avg\n","\n","\n","def predict(images, model, tokenizer, device):\n","    model.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model(images)\n","    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    text_preds = tokenizer.decode(pred)\n","    return text_preds\n","\n","\n","def get_loaders(tokenizer, config):\n","    train_transforms = get_train_transforms(\n","        height=config['image']['height'],\n","        width=config['image']['width'])\n","    train_loader = get_data_loader(\n","        json_path=config['train']['json_path'],\n","        root_path=config['train']['root_path'],\n","        transforms=train_transforms,\n","        tokenizer=tokenizer,\n","        batch_size=config['train']['batch_size'],\n","        drop_last=True)\n","    val_transforms = get_val_transforms(\n","        height=config['image']['height'],\n","        width=config['image']['width'])\n","    val_loader = get_data_loader(\n","        transforms=val_transforms,\n","        json_path=config['val']['json_path'],\n","        root_path=config['val']['root_path'],\n","        tokenizer=tokenizer,\n","        batch_size=config['val']['batch_size'],\n","        drop_last=False)\n","    return train_loader, val_loader\n","\n","\n","def train(config):\n","    tokenizer = Tokenizer(config['alphabet'])\n","    os.makedirs(config['save_dir'], exist_ok=True)\n","    train_loader, val_loader = get_loaders(tokenizer, config)\n","\n","    model = CRNN(number_class_symbols=tokenizer.get_num_chars(), pretrained=True)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer=optimizer, mode='min', factor=0.3, patience=3)\n","    loss_best = np.inf\n","    for epoch in range(100):\n","        loss_avg = train_loop(train_loader, model, criterion, optimizer, scheduler, epoch, gradient_accumulation_steps=2)\n","        scheduler.step(loss_avg)\n","\n","        if loss_avg < loss_best:\n","            loss_best = loss_avg\n","            torch.save(model.state_dict(), os.path.join(config['save_dir'], f'recognition_V1.ckpt'))\n","            print('Model weights saved')"],"metadata":{"id":"fyVhoToEvzgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(config_json)"],"metadata":{"id":"dIgU8tauvzdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TtjuLlljvzQW"},"execution_count":null,"outputs":[]}]}