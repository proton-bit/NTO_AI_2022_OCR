{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17572,"status":"ok","timestamp":1646284842345,"user":{"displayName":"Дмитрий Куценко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlLmaITHRBO-8rtINbs35JitYwkme0zMpo2SpC-A=s64","userId":"04146547092238065676"},"user_tz":-300},"id":"EERz9etAaISU","outputId":"c445c51c-0429-47c2-eb46-eaea5d952222"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2hlLHZ7Ybb75"},"outputs":[],"source":["import zipfile\n","with zipfile.ZipFile('data_final.zip') as zf:\n","    zf.extractall('')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":248555,"status":"ok","timestamp":1646285217228,"user":{"displayName":"Дмитрий Куценко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlLmaITHRBO-8rtINbs35JitYwkme0zMpo2SpC-A=s64","userId":"04146547092238065676"},"user_tz":-300},"id":"hv7pyF8kbb5f","outputId":"2fc7fc94-7020-4e3d-a90a-30a4d538395f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 5.6 MB 17 kB/s \n","\u001b[K     |████████████████████████████████| 47 kB 3.9 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 34.4 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 75.0 MB/s \n","\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 708.0 MB 10 kB/s \n","\u001b[K     |████████████████████████████████| 5.9 MB 51.9 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.6.0+cu101 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.6.0+cu101 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 421.8 MB 22 kB/s \n","\u001b[K     |████████████████████████████████| 448 kB 90.6 MB/s \n","\u001b[K     |████████████████████████████████| 50 kB 9.2 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 50.0 MB/s \n","\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[K     |████████████████████████████████| 60.3 MB 96 kB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","\u001b[?25h  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!python -m pip install -qU detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n","!pip install -q torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install -q tensorflow==2.1.0\n","!pip install -q opencv-python==4.5.4.60\n","!pip install -q git+https://github.com/parlance/ctcdecode.git"]},{"cell_type":"code","source":["import json\n","import os\n","import sys\n","import warnings\n","\n","import cv2\n","import numpy as np\n","from PIL import Image, ImageDraw, ImageFont\n","from tqdm import tqdm\n","\n","import logging\n","\n","warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.engine import DefaultPredictor\n","from ctcdecode import CTCBeamDecoder\n","\n","logger = logging.getLogger(\"detectron2\")\n","logger.setLevel(logging.CRITICAL)\n","\n","# TEST_IMAGES_PATH, SAVE_PATH = sys.argv[1:]\n","TEST_IMAGES_PATH, SAVE_PATH = '/content/data_final/train_segmentation/images', 'prediction.json'\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","SEGM_MODEL_PATH = \"segmentation_3_2.pth\"\n","OCR_MODEL2_PATH = \"recognition_V4_3.pth\"\n","OCR_MODEL1_PATH = \"CRNN_SOS_V2_3.pth\"\n","RUS_TEXTS_PATH = 'rus_texts.txt'\n","ENG_TEXTS_PATH = 'eng_texts.txt'\n","RUS_BEAM_SEARCH_PATH = f\"rus_language_model.gz\"\n","ENG_BEAM_SEARCH_PATH = f\"eng_language_model.gz\"\n","\n","with open(RUS_TEXTS_PATH, 'r') as f:\n","    RUS_TEXTS = set(f.read().split('\\n'))\n","\n","with open(ENG_TEXTS_PATH, 'r') as f:\n","    ENG_TEXTS = set(f.read().split('\\n'))\n","\n","CONFIG_JSON = {\n","    \"alphabet\": ' !\"%\\'()+,-./0123456789:;=?DFGIJLNRSUVWY[]bdfghijlnqrstuvwyz|ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","    \"image\": {\"width\": 512, \"height\": 64}}\n","\n","\n","def get_contours_from_mask(mask, min_area=5):\n","    contours, hierarchy = cv2.findContours(\n","        mask.astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n","    contour_list = []\n","    for contour in contours:\n","        if cv2.contourArea(contour) >= min_area:\n","            contour_list.append(contour)\n","    return contour_list\n","\n","\n","def get_larger_contour(contours):\n","    larger_area = 0\n","    larger_contour = None\n","    for contour in contours:\n","        area = cv2.contourArea(contour)\n","        if area > larger_area:\n","            larger_contour = contour\n","            larger_area = area\n","\n","    \"\"\"\n","    Скругляем маску\n","    \"\"\"\n","    if larger_contour is not None:\n","        larger_contour = cv2.approxPolyDP(larger_contour, 0.01 * cv2.arcLength(larger_contour, True), True)\n","    return larger_contour\n","\n","\n","class SEGMpredictor:\n","    def __init__(self, model_path):\n","        cfg = get_cfg()\n","        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n","        cfg.MODEL.WEIGHTS = model_path\n","        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15\n","        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n","        cfg.INPUT.FORMAT = \"BGR\"\n","        cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n","        cfg.INPUT.MIN_SIZE_TEST = 1260\n","        cfg.INPUT.MAX_SIZE_TEST = 1680\n","\n","        self.small_predictor = DefaultPredictor(cfg)\n","        cfg.INPUT.MIN_SIZE_TEST = 816\n","        cfg.INPUT.MAX_SIZE_TEST = 1680\n","        self.big_predictor = DefaultPredictor(cfg)\n","\n","    def __call__(self, img):\n","        if max(img.shape[0], img.shape[1]) > min(img.shape[0], img.shape[1]) * 2:\n","            outputs = self.big_predictor(img)\n","        else:\n","            outputs = self.small_predictor(img)\n","        prediction = outputs[\"instances\"].pred_masks.cpu().numpy()\n","        contours = []\n","        for pred in prediction:\n","            contour_list = get_contours_from_mask(pred)\n","            contours.append(get_larger_contour(contour_list))\n","        return contours\n","\n","\n","def get_char_map(alphabet):\n","    char_map = {value: idx + 1 for (idx, value) in enumerate(alphabet)}\n","    char_map[\"<BLANK>\"] = 0\n","    return char_map\n","\n","\n","class Tokenizer:\n","    def __init__(self, alphabet):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        dec_words = []\n","        for word in enc_word_list:\n","            word_chars = \"\"\n","            for idx, char_enc in enumerate(word):\n","                if char_enc != self.char_map[\"<BLANK>\"] and not (idx > 0 and char_enc == word[idx - 1]):\n","                    word_chars += self.rev_char_map[char_enc]\n","            dec_words.append(word_chars)\n","        return dec_words\n","\n","\n","class TokenizerBS:\n","    def __init__(self, alphabet, beam_search_path):\n","        self.char_map = get_char_map(alphabet)\n","        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n","\n","        self.beam_search = CTCBeamDecoder(\n","            '_ !\"%\\'()+,-./0123456789:;=?DFGIJLNRSUVWY[]bdfghijlnqrstuvwyz|ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№',\n","            model_path=beam_search_path,\n","            alpha=0.8, beta=3.0,\n","            cutoff_top_n=40, cutoff_prob=1.0,\n","            beam_width=70, num_processes=2,\n","            blank_id=0, log_probs_input=True)\n","\n","    def get_num_chars(self):\n","        return len(self.char_map)\n","\n","    def decode(self, enc_word_list):\n","        beam_results, beam_scores, time_steps, out_lens = self.beam_search.decode(enc_word_list)\n","        text_preds = ['' for x in range(len(beam_results))]\n","        for i in range(len(beam_results)):\n","            hyp_len = out_lens[i][0]\n","            for x in range(int(hyp_len)):\n","                if beam_results[i, 0, x] > 0:\n","                    text_preds[i] += \\\n","                    '_ !\"%\\'()+,-./0123456789:;=?DFGIJLNRSUVWY[]bdfghijlnqrstuvwyz|ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'[\n","                        beam_results[i, 0, x]]\n","        return text_preds\n","\n","\n","class Normalize:\n","    def __call__(self, img):\n","        img = img.astype(np.float32) / 255\n","        return img\n","\n","\n","class ToTensor:\n","    def __call__(self, arr):\n","        arr = torch.from_numpy(arr)\n","        return arr\n","\n","\n","class MoveChannels:\n","    def __init__(self, to_channels_first=True):\n","        self.to_channels_first = to_channels_first\n","\n","    def __call__(self, image):\n","        if self.to_channels_first:\n","            return np.moveaxis(image, -1, 0)\n","        else:\n","            return np.moveaxis(image, 0, -1)\n","\n","\n","class ImageResize:\n","    def __init__(self, height, width):\n","        self.height = height\n","        self.width = width\n","\n","    def __call__(self, image):\n","        h, w, c = image.shape\n","        new_image = np.zeros((self.height, self.width, c), np.uint8)\n","        scale = self.height / h\n","        if int(w * scale) <= self.width:\n","            image = cv2.resize(image, (int(w * scale), self.height), interpolation=cv2.INTER_LINEAR)\n","            new_image[:, :image.shape[1], :] = image\n","        else:\n","            new_height = int(self.height * (self.width / int(w * scale)))\n","            image = cv2.resize(image, (self.width, new_height), interpolation=cv2.INTER_LINEAR)\n","            new_image[(self.height - new_height) // 2:-((self.height - new_height) - (self.height - new_height) // 2),\n","            :image.shape[1], :] = image\n","\n","        return new_image\n","\n","\n","def get_val_transforms(height, width):\n","    transforms = torchvision.transforms.Compose(\n","        [\n","            ImageResize(height, width),\n","            MoveChannels(to_channels_first=True),\n","            Normalize(),\n","            ToTensor(),\n","        ]\n","    )\n","    return transforms\n","\n","\n","def get_resnet34_new_backbone(pretrained=True):\n","    m = torchvision.models.resnet34(pretrained=False)\n","    input_conv = nn.Conv2d(3, 64, 7, 1, 3)\n","    blocks = [input_conv, m.bn1, m.relu,\n","              m.maxpool, m.layer1, m.layer2, m.layer3]\n","    return nn.Sequential(*blocks)\n","\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(\n","            input_size, hidden_size, num_layers,\n","            dropout=dropout, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        return out\n","\n","\n","class CRNN(nn.Module):\n","    def __init__(self, number_class_symbols, time_feature_count=256, lstm_hidden=400, lstm_len=3, pretrained=True):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_new_backbone(pretrained=pretrained)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((time_feature_count, time_feature_count))\n","        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2, 300),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(300, number_class_symbols))\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x\n","\n","\n","class CRNN_big(nn.Module):\n","    def __init__(self, number_class_symbols, time_feature_count=256, lstm_hidden=512, lstm_len=3, pretrained=True):\n","        super().__init__()\n","        self.feature_extractor = get_resnet34_new_backbone(pretrained=pretrained)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((lstm_hidden, time_feature_count))\n","        self.bilstm = BiLSTM(lstm_hidden, lstm_hidden, lstm_len)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(lstm_hidden * 2, 300),\n","            nn.GELU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(300, number_class_symbols))\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        b, c, h, w = x.size()\n","        x = x.view(b, c * h, w)\n","        x = self.avg_pool(x)\n","        x = x.transpose(1, 2)\n","        x = self.bilstm(x)\n","        x = self.classifier(x)\n","        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n","        return x\n","\n","\n","\"\"\"\n","Узнаем на каком языке пишут\n","\"\"\"\n","def get_language(preds_grid):\n","    english_letters = 'DFGJLNRSUVWYbdfghijlnqrstuvwyz'\n","    russian_letters = 'ЁБГДЖЗИЛПФЦЧШЩЭЮЯбвгджзийлнптуфцчшщъыьэюяё'\n","    eng, rus = 0, 0\n","    for word in preds_grid:\n","        for let in word:\n","            if let in english_letters:\n","                eng += 1\n","            elif let in russian_letters:\n","                rus += 1\n","    if eng > rus:\n","        return 'eng'\n","    else:\n","        return 'rus'\n","\n","\n","\"\"\"\n","Если обе CRNN дают одинаковый выход, то его выбираем, иначе смотрим существует ли слово в нашем list слов\n","\"\"\"\n","def predict(images, model1, model2, tokenizer, tokenizer_bs_rus, tokenizer_bs_eng, device):\n","    model1.eval()\n","    model2.eval()\n","    images = images.to(device)\n","    with torch.no_grad():\n","        output = model1(images)\n","        output = output.detach().cpu()\n","        out_for_bs = output.transpose(0, 1)\n","        out_1 = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","\n","        output = model2(images)\n","        output = output.detach().cpu()\n","        out_2 = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n","    preds_grid1 = tokenizer.decode(out_1)\n","    preds_grid2 = tokenizer.decode(out_2)\n","    language = get_language(preds_grid1)\n","    if language == 'eng':\n","        preds_beam = tokenizer_bs_eng.decode(out_for_bs)\n","    else:\n","        preds_beam = tokenizer_bs_rus.decode(out_for_bs)\n","\n","    final_preds = []\n","    for i in range(len(preds_grid1)):\n","        if language == 'eng':\n","            if preds_grid1[i] in ENG_TEXTS or preds_grid1[i] == preds_grid2[i] or (\n","                    preds_grid2[i] not in ENG_TEXTS and preds_beam[i] not in ENG_TEXTS):\n","                final_preds.append(preds_grid1[i])\n","            elif preds_grid2[i] in ENG_TEXTS:\n","                final_preds.append(preds_grid2[i])\n","            else:\n","                final_preds.append(preds_beam[i])\n","        else:\n","            if preds_grid1[i] in RUS_TEXTS or preds_grid1[i] == preds_grid2[i] or (\n","                    preds_grid2[i] not in RUS_TEXTS and preds_beam[i] not in RUS_TEXTS):\n","                final_preds.append(preds_grid1[i])\n","            elif preds_grid2[i] in RUS_TEXTS:\n","                final_preds.append(preds_grid2[i])\n","            else:\n","                final_preds.append(preds_beam[i])\n","\n","    final_preds = language_sort(final_preds, language)\n","    return final_preds\n","\n","\n","class InferenceTransform:\n","    def __init__(self, height, width):\n","        self.transforms = get_val_transforms(height, width)\n","\n","    def __call__(self, images):\n","        transformed_images = []\n","        for image in images:\n","            image = self.transforms(image)\n","            transformed_images.append(image)\n","        transformed_tensor = torch.stack(transformed_images, 0)\n","        return transformed_tensor\n","\n","\n","class OcrPredictor:\n","    def __init__(self, model1_path, model2_path, config, rus_beam_search_path, eng_beam_search_path, device=\"cuda\"):\n","        self.tokenizer_bs_rus = TokenizerBS(config[\"alphabet\"], rus_beam_search_path)\n","        self.tokenizer_bs_eng = TokenizerBS(config[\"alphabet\"], eng_beam_search_path)\n","        self.tokenizer = Tokenizer(config['alphabet'])\n","        self.device = torch.device(device)\n","        # load model\n","        self.model1 = CRNN_big(number_class_symbols=self.tokenizer.get_num_chars())\n","        self.model1.load_state_dict(torch.load(model1_path))\n","        self.model1.to(self.device)\n","\n","        self.model2 = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n","        self.model2.load_state_dict(torch.load(model2_path))\n","        self.model2.to(self.device)\n","\n","        self.transforms = InferenceTransform(\n","            height=config[\"image\"][\"height\"],\n","            width=config[\"image\"][\"width\"])\n","\n","    def __call__(self, images):\n","        if isinstance(images, (list, tuple)):\n","            one_image = False\n","        elif isinstance(images, np.ndarray):\n","            images = [images]\n","            one_image = True\n","        else:\n","            raise Exception(\n","                f\"Input must contain np.ndarray, \"\n","                f\"tuple or list, found {type(images)}.\")\n","\n","        images = self.transforms(images)\n","        text_preds = predict(images, self.model1, self.model2, self.tokenizer, self.tokenizer_bs_rus,\n","                             self.tokenizer_bs_eng, self.device)\n","\n","        return text_preds\n","\n","\n","def crop_img_by_polygon(img, polygon):\n","    pts = np.array(polygon)\n","    rect = cv2.boundingRect(pts)\n","    x, y, w, h = rect\n","    croped = img[y: y + h, x: x + w].copy()\n","    pts = pts - pts.min(axis=0)\n","    mask = np.zeros(croped.shape[:2], np.uint8)\n","    cv2.drawContours(mask, [pts], -1, (255, 255, 255), -1, cv2.LINE_AA)\n","    dst = cv2.bitwise_and(croped, croped, mask=mask)\n","    return dst\n","\n","\n","\"\"\"\n","Фильтруем prediction CRNN моделей. Если пишут на английском, то переводим prediction в кириллицу\n","\"\"\"\n","def language_sort(pred_texts, language):\n","    english_letters = 'DFGJLNRSUVWYbdfghijlnqrstuvwyz'\n","    russian_letters = 'ЁБГДЖЗИЛПФЦЧШЩЭЮЯбгджзийлнпуфцчшщъыьэюяё'\n","\n","    if language == 'eng':\n","        english_conv = {' ': ' ', '!': '!', '\"': '\"', '%': '%', \"'\": \"'\", '(': '(', ')': ')', '+': '+', ',': ',',\n","                        '-': '-', '.': '.', '/': '/', '0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5',\n","                        '6': '6', '7': '7', '8': '8', '9': '9', ':': ':', ';': ';', '=': '=', '?': '?', 'D': 'D',\n","                        'F': 'F', 'G': 'G', 'I': 'I', 'J': 'J', 'L': 'L', 'N': 'N', 'R': 'R', 'S': 'S', 'U': 'U',\n","                        'V': 'V', 'W': 'W', 'Y': 'Y', '[': '[', ']': ']', '_': '_', 'b': 'b', 'd': 'd', 'f': 'f',\n","                        'g': 'g', 'h': 'h', 'i': 'i', 'j': 'j', 'l': 'l', 'n': 'n', 'q': 'q', 'r': 'r', 's': 's',\n","                        't': 't', 'u': 'u', 'v': 'v', 'w': 'w', 'y': 'y', 'z': 'z', '|': '|', 'Ё': 'Ё', 'А': 'A',\n","                        'Б': 'Б', 'В': 'B', 'Г': 'Г', 'Д': 'Д', 'Е': 'E', 'Ж': 'Ж', 'З': 'З', 'И': 'И', 'К': 'K',\n","                        'Л': 'Л', 'М': 'M', 'Н': 'H', 'О': 'O', 'П': 'П', 'Р': 'P', 'С': 'C', 'Т': 'T', 'У': 'Y',\n","                        'Ф': 'Ф', 'Х': 'X', 'Ц': 'Ц', 'Ч': 'Ч', 'Ш': 'Ш', 'Щ': 'Щ', 'Э': 'Э', 'Ю': 'Ю', 'Я': 'Я',\n","                        'а': 'a', 'б': 'б', 'в': 'B', 'г': 'г', 'д': 'д', 'е': 'e', 'ж': 'ж', 'з': 'з', 'и': 'и',\n","                        'й': 'й', 'к': 'k', 'л': 'л', 'м': 'м', 'н': 'н', 'о': 'o', 'п': 'п', 'р': 'p', 'с': 'c',\n","                        'т': 'T', 'у': 'y', 'ф': 'ф', 'х': 'x', 'ц': 'ц', 'ч': 'ч', 'ш': 'ш', 'щ': 'щ', 'ъ': 'ъ',\n","                        'ы': 'ы', 'ь': 'ь', 'э': 'э', 'ю': 'ю', 'я': 'я', 'ё': 'ё', '№': '№'}\n","        new_outputs = []\n","        for word in pred_texts:\n","            new_outputs.append(''.join(list(english_conv[x] for x in word if x not in russian_letters)))\n","        pred_texts = new_outputs\n","    else:\n","        new_outputs = []\n","        for word in pred_texts:\n","            new_outputs.append(''.join(list(x for x in word if x not in english_letters)))\n","    return pred_texts\n","\n","\n","class PiepleinePredictor:\n","    def __init__(self, segm_model_path, ocr_model1_path, ocr_model2_path, ocr_config, rus_beam_search_path,\n","                 eng_beam_search_path):\n","        self.segm_predictor = SEGMpredictor(model_path=segm_model_path)\n","        self.ocr_predictor = OcrPredictor(\n","            model1_path=ocr_model1_path,\n","            model2_path=ocr_model2_path,\n","            config=CONFIG_JSON,\n","            rus_beam_search_path=rus_beam_search_path,\n","            eng_beam_search_path=eng_beam_search_path\n","        )\n","\n","    def __call__(self, img):\n","        output = {'predictions': []}\n","        contours = self.segm_predictor(img)\n","        crops = []\n","        new_contours = []\n","        for contour in contours:\n","            if contour is not None:\n","                crop = crop_img_by_polygon(img, contour)\n","                crops.append(crop)\n","                new_contours.append(contour)\n","\n","        pred_texts = self.ocr_predictor(crops)\n","        for j in range(len(pred_texts)):\n","            if len(pred_texts[j]):\n","                output['predictions'].append(\n","                    {'polygon': [[int(i[0][0]), int(i[0][1])] for i in new_contours[j]],\n","                     'text': pred_texts[j]})\n","\n","        return output\n","\n","\n","# def main():\n","#     pipeline_predictor = PiepleinePredictor(\n","#         segm_model_path=SEGM_MODEL_PATH,\n","#         ocr_model1_path=OCR_MODEL1_PATH,\n","#         ocr_model2_path=OCR_MODEL2_PATH,\n","#         ocr_config=CONFIG_JSON,\n","#         rus_beam_search_path=RUS_BEAM_SEARCH_PATH,\n","#         eng_beam_search_path=ENG_BEAM_SEARCH_PATH\n","#     )\n","\n","#     pred_data = {}\n","#     for img_name in tqdm(os.listdir(TEST_IMAGES_PATH)):\n","#         image = cv2.imread(os.path.join(TEST_IMAGES_PATH, img_name))\n","#         pred_data[img_name] = pipeline_predictor(image)\n","\n","#     with open(SAVE_PATH, \"w\") as f:\n","#         json.dump(pred_data, f)\n","\n","\n","# if __name__ == \"__main__\":\n","#     main()\n"],"metadata":{"id":"jWEuYCmrLzS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxS6UPRtrk-K"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","def get_image_visualization(img, pred_data, fontpath, font_koef=50):\n","    h, w = img.shape[:2]\n","    font = ImageFont.truetype(fontpath, int(h/font_koef))\n","    empty_img = Image.new('RGB', (w, h), (255, 255, 255))\n","    draw = ImageDraw.Draw(empty_img)\n","\n","    for prediction in pred_data['predictions']:\n","        polygon = prediction['polygon']\n","        pred_text = prediction['text']\n","        cv2.drawContours(img, np.array([polygon]), -1, (0, 255, 0), 2)\n","        x, y, w, h = cv2.boundingRect(np.array([polygon]))\n","        draw.text((x, y), pred_text, fill=0, font=font)\n","\n","    vis_img = np.array(empty_img)\n","    vis = np.concatenate((img, vis_img), axis=1)\n","    return vis"]},{"cell_type":"code","source":["pipeline_predictor = PiepleinePredictor(\n","    segm_model_path=SEGM_MODEL_PATH,\n","    ocr_model1_path=OCR_MODEL1_PATH,\n","    ocr_model2_path=OCR_MODEL2_PATH,\n","    ocr_config=CONFIG_JSON,\n","    rus_beam_search_path=RUS_BEAM_SEARCH_PATH,\n","    eng_beam_search_path=ENG_BEAM_SEARCH_PATH\n",")"],"metadata":{"id":"xuVSVnHnPPDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1u97HWysmblWs0zmkvg8Dz4DcIev0uZix"},"executionInfo":{"elapsed":22387,"status":"ok","timestamp":1646285314485,"user":{"displayName":"Дмитрий Куценко","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhlLmaITHRBO-8rtINbs35JitYwkme0zMpo2SpC-A=s64","userId":"04146547092238065676"},"user_tz":-300},"id":"9B1O4uXZrQjl","outputId":"c7e9fd56-a481-4011-cb32-7ec6fa19d559"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["img = cv2.imread('/content/data_final/train_segmentation/images/2_2_eng.jpg')\n","output = pipeline_predictor(img)\n","\n","vis = get_image_visualization(img, output, 'font.otf')\n","\n","plt.figure(figsize=(40, 40))\n","plt.imshow(vis)\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Final_INFERENCE.ipynb","provenance":[],"authorship_tag":"ABX9TyPyFLvFo4afRkMBL9D4ZDSw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}